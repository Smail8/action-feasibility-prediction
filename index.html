<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Learning to Predict Action Feasibility for Task and Motion Planning in 3D Environments - Smail Ait Bouhsain">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="We propose an efficient method for 3D scene representation, along with a deep neural network capable of predicting the probability of feasibility of an action in task and motion planning. We demonstrate the performance gain of using our approach on multiple problem domains, significantly reducing geometric planning time.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="task and motion planning, TAMP, 3D environments, action feasibility prediction, deep neural networks, robotics, scene representation, manipulation planning, geometric planning, machine learning, artificial intelligence">
  <!-- TODO: List all authors -->
  <meta name="author" content="Smail Ait Bouhsain, Rachid Alami, Thierry Siméon">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="LAAS-CNRS">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Learning to Predict Action Feasibility for Task and Motion Planning in 3D Environments">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="We propose an efficient method for 3D scene representation, along with a deep neural network capable of predicting the probability of feasibility of an action in task and motion planning. We demonstrate the performance gain of using our approach on multiple problem domains, significantly reducing geometric planning time.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://github.com/Smail8/afpnet.github.io/static/images/panda_icon.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Learning to Predict Action Feasibility for Task and Motion Planning in 3D Environments - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Smail Ait Bouhsain">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="task and motion planning">
  <meta property="article:tag" content="action feasibility prediction">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@LaasCNRS">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@Sm1Le8B">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="Learning to Predict Action Feasibility for Task and Motion Planning in 3D Environments">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="We propose an efficient method for 3D scene representation, along with a deep neural network capable of predicting the probability of feasibility of an action in task and motion planning. We demonstrate the performance gain of using our approach on multiple problem domains, significantly reducing geometric planning time.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://github.com/Smail8/afpnet.github.io/static/images/panda_icon.png">
  <meta name="twitter:image:alt" content="Learning to Predict Action Feasibility for Task and Motion Planning in 3D Environments - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Learning to Predict Action Feasibility for Task and Motion Planning in 3D Environments">
  <meta name="citation_author" content="Ait Bouhsain, Smail">
  <meta name="citation_author" content="Alami, Rachid">
  <meta name="citation_author" content="Siméon, Thierry">
  <meta name="citation_publication_date" content="2023">
  <meta name="citation_conference_title" content="IEEE International Conference on Robotics and Automation (ICRA)">
  <meta name="citation_pdf_url" content="https://github.com/Smail8/afpnet.github.io/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Learning to Predict Action Feasibility for Task and Motion Planning in 3D Environments - Ait Bouhsain, Smail, Alami, Rachid, Siméon, Thierry | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/panda_icon.png">
  <link rel="apple-touch-icon" href="static/images/panda_icon.png">

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Learning to Predict Action Feasibility for Task and Motion Planning in 3D Environments",
    "description": "This paper presents a novel approach to predicting action feasibility in 3D environments, significantly improving task and motion planning.",
    "author": [
      {
        "@type": "Person",
        "name": "Smail Ait Bouhsain",
        "affiliation": {
          "@type": "Organization",
          "name": "LAAS-CNRS"
        }
      },
      {
        "@type": "Person",
        "name": "Rachid Alami",
        "affiliation": {
          "@type": "Organization",
          "name": "LAAS-CNRS"
        }
      },
      {
        "@type": "Person",
        "name": "Thierry Siméon",
        "affiliation": {
          "@type": "Organization",
          "name": "LAAS-CNRS"
        }
      }
    ],
    "datePublished": "2023-05-29",
    "publisher": {
      "@type": "Organization",
      "name": "IEEE International Conference on Robotics and Automation (ICRA)"
    },
    "url": "https://afpnet.github.io",
    "image": "https://afpnet.github.io/static/images/social_preview.png",
    "keywords": [
      "task and motion planning",
      "TAMP",
      "3D environments",
      "action feasibility prediction",
      "deep neural networks",
      "robotics",
      "scene representation",
      "manipulation planning",
      "geometric planning",
      "machine learning",
      "artificial intelligence"
    ],
    "abstract": 
    "In Task and motion planning (TAMP), symbolic search is combined with continuous geometric planning. A task planner finds an action sequence while a motion planner checks its feasibility and plans the corresponding sequence of motions. However, due to the high combinatorial complexity of discrete search, the number of calls to the geometric planner can be very large. Previous works [1] [2] leverage learning methods to efficiently predict the feasibility of actions, much like humans do, on tabletop scenarios. This way, the time spent on motion planning can be greatly reduced. In this work, we generalize these methods to 3D environments, thus covering the whole workspace of the robot. We propose an efficient method for 3D scene representation, along with a deep neural network capable of predicting the probability of feasibility of an action. We develop a simple TAMP algorithm that integrates the trained classifier, and demonstrate the performance gain of using our approach on multiple problem domains. On complex problems, our method can reduce the time spent on geometric planning by up to 90%.",
    "citation": 
    "@inproceedings{ait2023learning,
      title={Learning to predict action feasibility for task and motion planning in 3d environments},
      author={Ait Bouhsain, Smail and Alami, Rachid and Simeon, Thierry},
      booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
      pages={3736--3742},
      year={2023},
      organization={IEEE}
    }",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://afpnet.github.io"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "AI for Robotics"
      },
      {
        "@type": "Thing", 
        "name": "Machine Learning"
      },
      {
        "@type": "Thing", 
        "name": "Task and Motion Planning"
      },
      {
        "@type": "Thing", 
        "name": "Deep Learning"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "LAAS-CNRS",
    "url": "https://www.laas.fr",
    "logo": "https://github.com/Smail8/afpnet.github.io/static/images/laas_logo.png",
    "sameAs": [
      "https://twitter.com/LaasCNRS",
      "https://github.com/Smail8"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- TODO: Replace with your lab's related works -->
        <a href="" class="work-item" target="_blank">
          <div class="work-info">
            <!-- TODO: Replace with actual paper title -->
            <h5>Simultaneous Action and Grasp Feasibility Prediction for Task and Motion Planning through Multi-Task Learning</h5>
            <!-- TODO: Replace with brief description -->
            <p>We propose AGFP-Net, a multi-task neural network that predicts action feasibility and grasp type feasibility, significantly improving task and motion planning (TAMP) by reducing geometric planning time and solving more complex problems.</p>
            <!-- TODO: Replace with venue and year -->
            <span class="work-venue">2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023)</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <!-- TODO: Add more related works or remove extra items -->
        <a href="" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Extending Task and Motion Planning with Feasibility Prediction: Towards Multi-Robot Manipulation Planning of Realistic Objects</h5>
            <p>We introduce a multi-robot TAMP algorithm that leverages feasibility prediction to address complex manipulation tasks. Our approach extends previous methods by enabling the handling of mesh-shaped objects and collaborative multi-robot settings, demonstrating significant improvements over baseline methods.</p>
            <span class="work-venue">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024)</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Learning Geometric Reasoning Networks for Robot Task and Motion Planning</h5>
            <p>We present Geometric Reasoning Networks (GRN), a graph neural network-based model for predicting action and grasp feasibility in Task and Motion Planning (TAMP). GRN reduces reliance on geometric planners by incorporating interpretability mechanisms such as inverse kinematics feasibility prediction and grasp obstruction estimation, enabling efficient and explainable planning in complex 3D environments.</p>
            <span class="work-venue">The Thirteenth International Conference on Learning Representations (ICLR 2025)</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">Learning to Predict Action Feasibility for Task and Motion Planning in 3D Environments</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://smail8.github.io" target="_blank">Smail Ait Bouhsain</a>,</span>
                <span class="author-block">
                  <a href="https://www.laas.fr/en/homepages/rachid/" target="_blank">Rachid Alami</a>,</span>
                  <span class="author-block">
                    <a href="https://www.laas.fr/fr/annuaire/29" target="_blank">Thierry Siméon</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block">LAAS-CNRS<br>IEEE International Conference on Robotics and Automation (ICRA 2023)</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://ieeexplore.ieee.org/abstract/document/10161114" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <span class="link-block">
                      <a href="https://drive.google.com/file/d/1ybnuKUjecRf6z9gSl5QmZ0B9ho9FH43I/view?usp=sharing" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-photo-video"></i>
                      </span>
                      <span>Poster</span>
                    </a>
                  </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://laas.hal.science/hal-03808885v2/document" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-hal"></i>
                  </span>
                  <span>HAL</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- TODO: Replace with your teaser video -->
      <iframe width="100%" height="500px" src="https://www.youtube.com/embed/lUs3DGHrSDE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      <!-- TODO: Replace with your video description -->
      <h2 class="subtitle has-text-centered">
        Given the state of a 3D environment, a high-level action, and an object to manipulate, our method predicts the geometric feasibility of the action. These predictions are then used as a heuristic to guide task and motion planning, by prioritizing the most promising actions first. This approach significantly reduces the time spent on geometric planning, enabling the robot to solve manipulation tasks more efficiently in 3D environments.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            In Task and motion planning (TAMP), symbolic search is combined with continuous geometric planning. A task planner finds an action sequence while a motion planner checks its feasibility and plans the corresponding sequence of motions. However, due to the high combinatorial complexity of discrete search, the number of calls to the geometric planner can be very large. Previous works [1] [2] leverage learning methods to efficiently predict the feasibility of actions, much like humans do, on tabletop scenarios. This way, the time spent on motion planning can be greatly reduced. In this work, we generalize these methods to 3D environments, thus covering the whole workspace of the robot. We propose an efficient method for 3D scene representation, along with a deep neural network capable of predicting the probability of feasibility of an action. We develop a simple TAMP algorithm that integrates the trained classifier, and demonstrate the performance gain of using our approach on multiple problem domains. On complex problems, our method can reduce the time spent on geometric planning by up to 90%.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Neural Network Image -->
<section class="section hero is-small">
  <div class="hero-body">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Proposed Model</h2>
        <figure class="image">
          <!-- TODO: Replace with the actual image path -->
          <img src="static/images/model.png" alt="Diagram of the proposed neural network">
        </figure>
        <p class="subtitle">
          A visualization of the proposed neural network architecture used for action feasibility prediction.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Neural Network Image -->

<!-- Benchmarks Section -->
<section class="section hero is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Benchmarks</h2>
      <div class="columns is-centered">
        <div class="column is-half has-text-centered">
          <figure class="image"></figure>
            <!-- TODO: Replace with the actual image path -->
            <img src="static/images/reorder.png" alt="Benchmark 1">
          </figure>
            <p class="subtitle"><strong>Reorder problem:</strong> Objects initially placed on a two-shelf cupboard must be moved to another cupboard in a different order, testing the algorithm's ability to handle grasp choices and infeasible placements under shelves.</p>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-half has-text-centered">
          <figure class="image"></figure>
            <!-- TODO: Replace with the actual image path -->
            <img src="static/images/unpack.png" alt="Benchmark 2">
          </figure>
            <p class="subtitle"><strong>Unpack problem:</strong> Objects initially placed on a tray-like surface must be unpacked and ordered into a cupboard. The challenge lies in selecting feasible grasps due to the proximity of objects and the restricted goal placements under a shelf, which disallow top grasps.</p>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-half has-text-centered">
          <figure class="image"></figure>
            <!-- TODO: Replace with the actual image path -->
            <img src="static/images/swap.png" alt="Benchmark 3">
          </figure>
          <p class="subtitle"><strong>Swap problem:</strong> A set of objects are initially placed either on the table or on higher support surfaces, and the goal is to swap their poses. The challenge arises from the fact that the goal poses of objects are already occupied, requiring the use of intermediary placements to achieve the desired configuration.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Benchmarks Section -->

<!-- Results Section -->
<section class="section hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Results</h2>
      <div class="columns is-centered">
        <!-- First image -->
        <div class="column is-half has-text-centered">
          <figure class="image">
            <img src="static/images/times_2obj.png" alt="Result 1">
          </figure>
          <p class="subtitle">Detailed planning time of our algorithm on the <strong>2-object</strong> version of the problems with and without using AFP-Net, averaged over 10 runs.</p>
        </div>
        <!-- Second image -->
        <div class="column is-half has-text-centered">
          <figure class="image">
            <img src="static/images/times_5obj.png" alt="Result 2">
          </figure>
          <p class="subtitle">Detailed planning time of our algorithm on the <strong>5-object</strong> version of the problems with and without using AFP-Net, averaged over 10 runs.</p>
        </div>
      </div>
      <!-- Add space between the first two figures and the last -->
      <div class="columns is-centered" style="margin-top: 2rem;"></div>
      <div class="columns is-centered">
        <!-- Third image -->
        <div class="column has-text-centered">
          <figure class="image">
            <img src="static/images/results.png" alt="Result 3">
          </figure>
          <p class="subtitle">Comparison of the performance of the TAMP algorithm with and without using AFP-Net on the 5-object version of the three problem domains. Results are averaged over 10 runs.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Results Section -->

<!-- Visualizations Section -->
<section class="section hero is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Visualizations</h2>
      <div class="columns is-centered">
        <!-- First GIF -->
        <div class="column is-one-third has-text-centered">
          <figure class="image">
            <img src="static/images/reorder5.gif" alt="Visualization 1">
          </figure>
          <p class="subtitle">Execution of the reordering task with 5 objects.</p>
        </div>
        <!-- Second GIF -->
        <div class="column is-one-third has-text-centered">
          <figure class="image">
            <img src="static/images/unpack5.gif" alt="Visualization 2">
          </figure>
          <p class="subtitle">Execution of the unpacking task with 5 objects.</p>
        </div>
        <!-- Third GIF -->
        <div class="column is-one-third has-text-centered">
          <figure class="image">
            <img src="static/images/swap5.gif" alt="Visualization 3">
          </figure>
          <p class="subtitle">Execution of the swapping task with 5 objects.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Visualizations Section -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{ait2023learning,
  title={Learning to predict action feasibility for task and motion planning in 3d environments},
  author={Ait Bouhsain, Smail and Alami, Rachid and Simeon, Thierry},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={3736--3742},
  year={2023},
  organization={IEEE}
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- Default Statcounter code for AFPNet (ICRA 2023)
https://smail8.github.io/action-feasibility-prediction/ -->
<script type="text/javascript">
var sc_project=13197490; 
var sc_invisible=1; 
var sc_security="dc5a2c10"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js" async></script>
<noscript><div class="statcounter"><a title="Web Analytics Made Easy -
Statcounter" href="https://statcounter.com/" target="_blank"><img
class="statcounter" src="https://c.statcounter.com/13197490/0/dc5a2c10/1/"
alt="Web Analytics Made Easy - Statcounter"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->
    <!-- End of Statcounter Code -->

  </body>
  </html>
